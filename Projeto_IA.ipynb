{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40b90843"
      },
      "source": [
        "<img src=\"http://meusite.mackenzie.br/rogerio/mackenzie_logo/UPM.2_horizontal_vermelho.jpg\"  width=300, align=\"right\">\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Tradução Neural Customizada: Aplicação de NLP no Domínio Biomédico**\n",
        "---\n",
        "\n",
        "Atenção, podem ser que nem todas as tarefas sejam executadas no Colab (a aplicação por exemplo, pode estar hospedada no streamlit cloud). Mas a maior parte pode estar aqui ou ao menos indicada e comentada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Além disso a entrega deve incluir:\n",
        "\n",
        "1. **Um GitHub público do projeto**\n",
        "2. **Código completo e executável em um notebook Python (este template)**\n",
        "3. **Uma aplicação streamlit para consumo do modelo**\n",
        "4. **Um texto/artigo do projeto**\n",
        "5. **Um vídeo (link YouTube ou outro) de no máximo 3min de apresentação do projeto**\n",
        "\n",
        "Um **`readme.md`** no GitHub público do projeto deve indicar (um índice) cada uma dessas entregas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qR6kcPlTeV_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Links:**\n",
        "1.   **GitHub:** https://github.com/erikhsu08/projetoIA\n",
        "2.   **Código completo e executável:** esse notebook mesmo\n",
        "3. **Aplicação streamlit:** tivemos problemas para subir a aplicação pela web, e por esse motivo deixamos um passo a passo de como fazer para subir a aplicação localmente.\n",
        "4. **Texto/ Artigo do projeto:**  https://docs.google.com/document/d/1DYdHE_ONzrDVHjv4vHvMwX7NmycGEmOJ-vY0FbZKVSY/edit?usp=sharing\n",
        "5. **Vídeo:** https://www.youtube.com/watch?v=6DFBNSNQ4wY\n",
        "\n"
      ],
      "metadata": {
        "id": "riiSBMCl70V1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYx9D4GZA5o9"
      },
      "source": [
        "#@title **Identificação do Grupo**\n",
        "\n",
        "#@markdown Integrantes do Grupo, nome completo em orgem alfabética\n",
        "Aluno1 = '10403109, Erik Samuel Viana Hsu' #@param {type:\"string\"}\n",
        "Aluno2 = '10400995, Mateus Kenzo Iochimoto' #@param {type:\"string\"}\n",
        "Aluno3 = '10400764, Thiago Shihan Cardoso Toma' #@param {type:\"string\"}\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Assinale aqui a sua opção de Projeto\n",
        "Projeto = \"IA Aplicada a Documentos: Uso de Grandes Modelos de Linguagem Abertos\" #@param [\"IA Aplicada a Imagens: Uso de Modelos de Redes Neurais\", \"IA Aplicada a Documentos: Uso de Grandes Modelos de Linguagem Abertos\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-MbC50IHTmh3",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resumo**\n",
        "\n",
        "# **1. Objetivo do projeto**\n",
        "\n",
        "O objetivo deste projeto é construir um sistema de **tradução automática neural** do inglês para o português, utilizando modelos pré-treinados da biblioteca **Hugging Face Transformers**. A proposta é aplicar técnicas modernas de NLP para adaptar um modelo de tradução com dados personalizados, visando melhorar sua performance em um domínio específico.\n",
        "\n",
        "# **2. Fontes dos dados e dados originais (coletados)**\n",
        "Os dados utilizados neste projeto foram extraídos do **EMEA Parallel Corpus** (European Medicines Agency), um conjunto de frases paralelas em inglês e português, focado no domínio biomédico. Os arquivos utilizados foram:\n",
        "\n",
        "- `EMEA.en-pt.en` — Frases em inglês  \n",
        "- `EMEA.en-pt.pt` — Traduções correspondentes em português\n",
        "\n",
        "# 3. **Ferramentas/pacotes de IA a serem utilizados para a construção da solução**\n",
        "\n",
        "- **Transformers** — Para carregamento e fine-tuning do modelo `Helsinki-NLP/opus-mt-en-ROMANCE`\n",
        "- **Datasets** — Para manipulação e tokenização dos dados\n",
        "- **Evaluate** — Para cálculo da métrica `sacreBLEU`\n",
        "- **PyTorch** — Backend para o treinamento\n",
        "\n",
        "# 4. **Uma prévia dos resultados.**\n",
        "O modelo foi ajustado com 2000 pares de frases para treino e 500 para avaliação.\n",
        "\n",
        "Para testar a função de tradução, passamos uma frase em inglês como parâmetro de entrada e a saída é a frase traduzida em português. Por exemplo, se a frase for \"Keep medicine away from children\", a tradução será \"Manter os medicamentos longe das crianças\". Outras frases e entradas podem ser testadas.\n"
      ],
      "metadata": {
        "id": "yxYbSf6mVM7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apresentação dos dados**\n"
      ],
      "metadata": {
        "id": "ctroSu6jNABS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizaremos o corpus EMEA (European Medicines Agency), que contém traduções de bulas de medicamentos em vários idiomas. Os dados estão disponíveis publicamente via [OPUS](https://opus.nlpl.eu/EMEA.php)."
      ],
      "metadata": {
        "id": "hL-g-h18YrhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-EMEA/v3/moses/en-pt.txt.zip -O emea_en-pt.zip\n",
        "!unzip emea_en-pt.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjqE7zw5fBU",
        "outputId": "7c98ba6f-8790-43da-d9b9-f6aa999a18b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-01 23:24:15--  https://object.pouta.csc.fi/OPUS-EMEA/v3/moses/en-pt.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35242984 (34M) [application/zip]\n",
            "Saving to: ‘emea_en-pt.zip’\n",
            "\n",
            "emea_en-pt.zip      100%[===================>]  33.61M  9.20MB/s    in 3.7s    \n",
            "\n",
            "2025-06-01 23:24:19 (9.20 MB/s) - ‘emea_en-pt.zip’ saved [35242984/35242984]\n",
            "\n",
            "Archive:  emea_en-pt.zip\n",
            "replace EMEA.en-pt.en? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualização de algumas amostras\n",
        "with open(\"EMEA.en-pt.en\", encoding=\"utf-8\") as f_en, open(\"EMEA.en-pt.pt\", encoding=\"utf-8\") as f_pt:\n",
        "    for _ in range(5):\n",
        "        print(\"EN:\", f_en.readline().strip())\n",
        "        print(\"PT:\", f_pt.readline().strip())\n",
        "        print(\"---\")"
      ],
      "metadata": {
        "id": "Iy_B3tehZBXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparação e transformação dos dados**\n",
        "\n"
      ],
      "metadata": {
        "id": "GDzwn_5AMZ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install evaluate sacrebleu"
      ],
      "metadata": {
        "id": "5si6yrzMZLxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Função que carrega os dados personalizados de tradução a partir de 2 arquivos de texto\n",
        "def load_custom_data():\n",
        "    with open(\"EMEA.en-pt.en\", encoding=\"utf-8\") as f_en, open(\"EMEA.en-pt.pt\", encoding=\"utf-8\") as f_pt:\n",
        "        data = [{\"translation\": {\"en\": en.strip(), \"pt\": pt.strip()}} for en, pt in zip(f_en, f_pt)] # cria uma lista de dicionários, onde cada item é um par de tradução\n",
        "    return Dataset.from_list(data) # converte a lista de pares de tradução em um objeto Dataset do Hugging Face\n",
        "\n",
        "dataset = load_custom_data() # carrega o dataset personalizado com a função acima\n",
        "dataset = dataset.train_test_split(test_size=0.1) # divide o dataset em dois subconjuntos -> teste (10%) e treino (90%)\n",
        "dataset # exibe o dataset"
      ],
      "metadata": {
        "id": "3hAtTmqbhkny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tuning do modelo**\n"
      ],
      "metadata": {
        "id": "mnJHmydNNfl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa etapa, utilizamos o modelo `Helsinki-NLP/opus-mt-en-ROMANCE` (o -pt era público mas ficou privado, mas o ROMANCE contempla pt também) com ajustes finos baseados no nosso conjunto técnico (bula de remédios)."
      ],
      "metadata": {
        "id": "AJU-gnj1h7l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "# Define o nome do modelo base para tradução do inglês para as línguas românicas (inclui pt, es, fr, etc)\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\n",
        "\n",
        "#Carrega o tokenizer, transformando os textos em tokens\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Carrega o modelo pré treinado de tradução com base no nome fornecido\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "#Função que pré-processa os dados de entrada para o treinamento\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]] # extrai os textos em inglês da estrutura translation do dataset\n",
        "    targets = [ex[\"pt\"] for ex in examples[\"translation\"]] # extrai as traduções para o português\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\") # tokeniza os textos de entrada\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\") #tokeniza os textos-alvo\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"] # add os labels tokenizados ao dicionário de entrada\n",
        "    return model_inputs\n",
        "\n",
        "# Aplica a função de pré processamento a todo o dataset, de forma batched\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "XAeRE1v9hsPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo os argumentos / hiperparâmetros de treinamento\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True,\n",
        ")\n",
        "\n",
        "small_train_dataset = tokenized_dataset[\"train\"].select(range(2000)) # dataset de treino\n",
        "small_eval_dataset = tokenized_dataset[\"test\"].select(range(500)) #dataset de teste\n",
        "\n",
        "# Trainer com modelo de tradução Seq2Seq\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        ")\n",
        "\n",
        "trainer.train() # inicia o treinamento"
      ],
      "metadata": {
        "id": "FvUj8l1pjVxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Avaliação do modelo**\n",
        "\n"
      ],
      "metadata": {
        "id": "p1Evo4PmNhBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para avaliar a qualidade de tradução, usamos a métrica BLEU (sacrebleu)."
      ],
      "metadata": {
        "id": "nT7lkz3Pj_E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\") # carrega metrica sacreBLEU\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    # Descompacta os pares de (predições, rótulos/labels)\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # Decodifica as predições\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Substitui -100 pelo token de pad no rótulo\n",
        "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
        "\n",
        "    # Decodifica os rótulos reais (targets) para texto legível, ignorando tokens especiais\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # SacreBLEU espera lista de listas no campo `references`\n",
        "    return metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])"
      ],
      "metadata": {
        "id": "XkPz9hpFj-mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Consumo do modelo**"
      ],
      "metadata": {
        "id": "ViQfwNxkNj0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função para traduzir frases novas com o modelo fine-tuned"
      ],
      "metadata": {
        "id": "1Y32fMvFkTPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True) #tokeniza o texto de entrada\n",
        "    translated = model.generate(**inputs) # gera tradução usando o modelo carregado\n",
        "    return tokenizer.decode(translated[0], skip_special_tokens=True) # decodifica a saída gerada para texto legível\n",
        "\n",
        "# Exemplo de uso\n",
        "exemplo = \"Keep medicine away from children.\"\n",
        "print(\"Tradução:\", translate(exemplo))"
      ],
      "metadata": {
        "id": "IJO2QLqqlEv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando Modelo Treinado"
      ],
      "metadata": {
        "id": "PxOJiEXVYyKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"modelo_final\")\n",
        "tokenizer.save_pretrained(\"modelo_final\")"
      ],
      "metadata": {
        "id": "ldBQU47OYxdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r modelo_final.zip modelo_final"
      ],
      "metadata": {
        "id": "phA2bA6sZjvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Referências**\n",
        "\n",
        "- OPUS EMEA Corpus: https://opus.nlpl.eu/EMEA.php  \n",
        "- MarianMT - Hugging Face: https://huggingface.co/Helsinki-NLP/opus-mt-en-pt  \n",
        "- Hugging Face Transformers: https://huggingface.co/docs/transformers/index  \n",
        "- Streamlit: https://streamlit.io/"
      ],
      "metadata": {
        "id": "7LtXrRFr4hg3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crUBC3IQ3U_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BluFtfHuCGzm",
        "cellView": "form"
      },
      "source": [
        "#@title **Avaliação**\n",
        "GitHub = 10 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Implementacao_Model_Code = 7 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Aplicacao_Streamlit = 9 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Texto_Artigo  = 6 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Video = 7 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Geral = 7 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gqw7hUZHyle",
        "cellView": "form"
      },
      "source": [
        "#@title **Nota Final**\n",
        "\n",
        "nota = 2*GitHub + 4*Implementacao_Model_Code + 2*Aplicacao_Streamlit + 1*Texto_Artigo + 1*Video\n",
        "\n",
        "nota = nota / 10\n",
        "\n",
        "print(f'Nota final do trabalho {nota :.1f}')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "alunos = pd.DataFrame()\n",
        "\n",
        "lista_tia = []\n",
        "lista_nome = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  exec(\"if Aluno\" + str(i) + \" !='None':  lista = Aluno\" + str(i) + \".split(','); lista_tia.append(lista[0]); lista_nome.append(lista[1].upper())\")\n",
        "\n",
        "alunos['tia'] = lista_tia\n",
        "alunos['nome'] = lista_nome\n",
        "alunos['nota'] = np.round(nota,1)\n",
        "print()\n",
        "display(alunos)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}